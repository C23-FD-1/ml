{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba69206d",
   "metadata": {},
   "source": [
    "# MODELLING\n",
    "### Fraud Detection System Development using Deep Neural Network for Reported Transactional Data in DANA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cd688ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1b4b799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('user_data_clip99_minmax.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6cbcc1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_scammer</th>\n",
       "      <th>trx_date</th>\n",
       "      <th>report_date</th>\n",
       "      <th>registereddate</th>\n",
       "      <th>birthday</th>\n",
       "      <th>is_verified</th>\n",
       "      <th>aqc_freq_prepaid_mobile</th>\n",
       "      <th>aqc_mean_prepaid_mobile_amount</th>\n",
       "      <th>aqc_freq_topup</th>\n",
       "      <th>aqc_freq_topup_within_7d</th>\n",
       "      <th>...</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>gender_None</th>\n",
       "      <th>job_position_KARYAWAN</th>\n",
       "      <th>job_position_LAINNYA</th>\n",
       "      <th>job_position_PEGAWAI_NS</th>\n",
       "      <th>job_position_PELAJAR</th>\n",
       "      <th>job_position_RUMAH_TANGGA</th>\n",
       "      <th>job_position_SPESIALIS</th>\n",
       "      <th>job_position_TIDAK_KERJA</th>\n",
       "      <th>job_position_WIRASWASTA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.784107</td>\n",
       "      <td>0.714822</td>\n",
       "      <td>0.782440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.097656</td>\n",
       "      <td>0.343195</td>\n",
       "      <td>0.171118</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.772855</td>\n",
       "      <td>0.704503</td>\n",
       "      <td>0.566964</td>\n",
       "      <td>0.013855</td>\n",
       "      <td>1</td>\n",
       "      <td>0.128906</td>\n",
       "      <td>0.144773</td>\n",
       "      <td>0.379822</td>\n",
       "      <td>0.203704</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.448664</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.014068</td>\n",
       "      <td>1</td>\n",
       "      <td>0.105469</td>\n",
       "      <td>0.118935</td>\n",
       "      <td>0.067260</td>\n",
       "      <td>0.011574</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.913502</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.999405</td>\n",
       "      <td>0.026893</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012859</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.870605</td>\n",
       "      <td>0.827392</td>\n",
       "      <td>0.893155</td>\n",
       "      <td>0.053004</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064293</td>\n",
       "      <td>0.060185</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_scammer  trx_date  report_date  registereddate  birthday  is_verified  \\\n",
       "0           0  0.784107     0.714822        0.782440  0.000000            1   \n",
       "1           1  0.772855     0.704503        0.566964  0.013855            1   \n",
       "2           1  0.448664     0.269231        0.783333  0.014068            1   \n",
       "3           1  0.913502     0.884615        0.999405  0.026893            1   \n",
       "4           1  0.870605     0.827392        0.893155  0.053004            1   \n",
       "\n",
       "   aqc_freq_prepaid_mobile  aqc_mean_prepaid_mobile_amount  aqc_freq_topup  \\\n",
       "0                 0.097656                        0.343195        0.171118   \n",
       "1                 0.128906                        0.144773        0.379822   \n",
       "2                 0.105469                        0.118935        0.067260   \n",
       "3                 0.000000                        0.000000        0.012859   \n",
       "4                 0.000000                        0.000000        0.064293   \n",
       "\n",
       "   aqc_freq_topup_within_7d  ...  gender_Male  gender_None  \\\n",
       "0                  0.138889  ...            1            0   \n",
       "1                  0.203704  ...            0            0   \n",
       "2                  0.011574  ...            0            0   \n",
       "3                  0.013889  ...            1            0   \n",
       "4                  0.060185  ...            1            0   \n",
       "\n",
       "   job_position_KARYAWAN  job_position_LAINNYA  job_position_PEGAWAI_NS  \\\n",
       "0                      0                     0                        0   \n",
       "1                      0                     0                        0   \n",
       "2                      1                     0                        0   \n",
       "3                      1                     0                        0   \n",
       "4                      1                     0                        0   \n",
       "\n",
       "   job_position_PELAJAR  job_position_RUMAH_TANGGA  job_position_SPESIALIS  \\\n",
       "0                     0                          0                       0   \n",
       "1                     0                          0                       0   \n",
       "2                     0                          0                       0   \n",
       "3                     0                          0                       0   \n",
       "4                     0                          0                       0   \n",
       "\n",
       "   job_position_TIDAK_KERJA  job_position_WIRASWASTA  \n",
       "0                         0                        1  \n",
       "1                         0                        1  \n",
       "2                         0                        0  \n",
       "3                         0                        0  \n",
       "4                         0                        0  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.rename(columns={\"Unnamed: 0\": \"id\"})\n",
    "df = df.drop(columns = ['uid', 'id'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cdddf7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Train and Validation Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "SPLIT_SIZE = 0.9\n",
    "\n",
    "train_len = int(len(df) * SPLIT_SIZE)\n",
    "\n",
    "shuffled_files = df.sample(frac = 1, random_state = 1)\n",
    "\n",
    "test_set = shuffled_files[train_len:]\n",
    "train_set = shuffled_files[:train_len]\n",
    "\n",
    "y_train = train_set['is_scammer']\n",
    "x_train = train_set.drop(columns=['is_scammer'])\n",
    "y_test = test_set['is_scammer']\n",
    "x_test = test_set.drop(columns=['is_scammer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9cb6778c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = tf.convert_to_tensor(x_train, dtype=tf.float64), tf.convert_to_tensor(y_train, dtype=tf.float64)\n",
    "x_test, y_test = tf.convert_to_tensor(x_test, dtype=tf.float64), tf.convert_to_tensor(y_test, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f15169",
   "metadata": {},
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(64, activation='relu', input_shape=(x_train.shape[1])),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c743d6",
   "metadata": {},
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.001),\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15fb116",
   "metadata": {},
   "source": [
    "history = model.fit(train_generator,\n",
    "                    epochs=15,\n",
    "                    verbose=1,\n",
    "                    validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abfece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y_pred, y):\n",
    "    # Compute the log loss function\n",
    "    ce = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=y_pred)\n",
    "    return tf.reduce_mean(ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a69008",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(tf.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.built = False\n",
    "        \n",
    "    def __call__(self, x, train=True):\n",
    "        # Initialize the model parameters on the first call\n",
    "        if not self.built:\n",
    "            # Randomly generate the weights and the bias term\n",
    "            rand_w = tf.random.uniform(shape=[x.shape[-1], 1], seed=22)\n",
    "            rand_b = tf.random.uniform(shape=[], seed=22)\n",
    "            self.w = tf.Variable(rand_w)\n",
    "            self.b = tf.Variable(rand_b)\n",
    "            self.built = True\n",
    "        # Compute the model output\n",
    "        z = tf.add(tf.matmul(x, self.w), self.b)\n",
    "        z = tf.squeeze(z, axis=1)\n",
    "        if train:\n",
    "            return z\n",
    "        return tf.sigmoid(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7de073",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57615118",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_reg(x_train[:5], train=False)\n",
    "y_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e29539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(y_pred, thresh=0.5):\n",
    "    # Return a tensor with  `1` if `y_pred` > `0.5`, and `0` otherwise\n",
    "    return tf.cast(y_pred > thresh, tf.float32)\n",
    "\n",
    "def accuracy(y_pred, y):\n",
    "    # Return the proportion of matches between `y_pred` and `y`\n",
    "    y_pred = tf.math.sigmoid(y_pred)\n",
    "    y_pred_class = predict_class(y_pred)\n",
    "    check_equal = tf.cast(y_pred_class == y,tf.float32)\n",
    "    acc_val = tf.reduce_mean(check_equal)\n",
    "    return acc_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f25016",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_val))\n",
    "val_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e827a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "epochs = 200\n",
    "learning_rate = 0.001\n",
    "train_losses, test_losses = [], []\n",
    "train_accs, test_accs = [], []\n",
    "\n",
    "# Set up the training loop and begin training\n",
    "for epoch in range(epochs):\n",
    "    batch_losses_train, batch_accs_train = [], []\n",
    "    batch_losses_test, batch_accs_test = [], []\n",
    "\n",
    "    # Iterate over the training data\n",
    "    for x_batch, y_batch in train_dataset:\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred_batch = log_reg(x_batch)\n",
    "            batch_loss = log_loss(y_pred_batch, y_batch)\n",
    "        batch_acc = accuracy(y_pred_batch, y_batch)\n",
    "        # Update the parameters with respect to the gradient calculations\n",
    "        grads = tape.gradient(batch_loss, log_reg.variables)\n",
    "        for g,v in zip(grads, log_reg.variables):\n",
    "            v.assign_sub(learning_rate * g)\n",
    "        # Keep track of batch-level training performance\n",
    "        batch_losses_train.append(batch_loss)\n",
    "        batch_accs_train.append(batch_acc)\n",
    "\n",
    "    # Iterate over the testing data\n",
    "    for x_batch, y_batch in test_dataset:\n",
    "        y_pred_batch = log_reg(x_batch)\n",
    "        batch_loss = log_loss(y_pred_batch, y_batch)\n",
    "        batch_acc = accuracy(y_pred_batch, y_batch)\n",
    "        # Keep track of batch-level testing performance\n",
    "        batch_losses_test.append(batch_loss)\n",
    "        batch_accs_test.append(batch_acc)\n",
    "\n",
    "    # Keep track of epoch-level model performance\n",
    "    train_loss, train_acc = tf.reduce_mean(batch_losses_train), tf.reduce_mean(batch_accs_train)\n",
    "    test_loss, test_acc = tf.reduce_mean(batch_losses_test), tf.reduce_mean(batch_accs_test)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch: {epoch}, Training log loss: {train_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1bab57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), train_losses, label = \"Training loss\")\n",
    "plt.plot(range(epochs), val_losses, label = \"Testing loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Log loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Log loss vs training iterations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f9faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epochs), train_accs, label = \"Training accuracy\")\n",
    "plt.plot(range(epochs), test_accs, label = \"Testing accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.title(\"Accuracy vs training iterations\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final training log loss: {train_losses[-1]:.3f}\")\n",
    "print(f\"Final testing log Loss: {test_losses[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2901ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final training accuracy: {train_accs[-1]:.3f}\")\n",
    "print(f\"Final testing accuracy: {test_accs[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e96aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_confusion_matrix(y, y_classes, typ):\n",
    "    # Compute the confusion matrix and normalize it\n",
    "    plt.figure(figsize=(10,10))\n",
    "    confusion = sk_metrics.confusion_matrix(y.numpy(), y_classes.numpy())\n",
    "    confusion_normalized = confusion / confusion.sum(axis=1)\n",
    "    axis_labels = range(2)\n",
    "    ax = sns.heatmap(\n",
    "        confusion_normalized, xticklabels=axis_labels, yticklabels=axis_labels,\n",
    "        cmap='Blues', annot=True, fmt='.4f', square=True)\n",
    "    plt.title(f\"Confusion matrix: {typ}\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "\n",
    "y_pred_train, y_pred_test = log_reg(x_train_norm, train=False), log_reg(x_test_norm, train=False)\n",
    "train_classes, test_classes = predict_class(y_pred_train), predict_class(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bc4a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(y_train, train_classes, 'Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94395631",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_confusion_matrix(y_test, test_classes, 'Testing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb0d07",
   "metadata": {},
   "source": [
    "Source:\n",
    "https://www.tensorflow.org/guide/core/logistic_regression_core"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
